{"cells":[{"cell_type":"markdown","metadata":{"id":"nfoI4e8Jonwf"},"source":["## 0 - Introduction\n","\n","Welcome to the <span style=\"color:yellowgreen\">Foundations of Machine Learning</span> (ECE5984_41) course!\n","\n","This is the <span style=\"color:red\">3rd</span> lab practice. From now on, you will going to implement Softmax regression, MLP"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Packages \n","\n","You have to install and use below packages for HW#2.\n","- [numpy](https://www.numpy.org): Fundamental package for matrix computation with python.\n","- [matplotlib](https://matplotlib.org): Package for visualization of the graph with python.\n","- [scikit-learn](https://scikit-learn.org/stable/):  Python module for machine learning built on top of SciPy\n","\n","**Do not use other machine learning packages in this homework, e.g., *tensorflow*, *pytorch*, *jax*, etc.**"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1664845206070,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"vx1DJBu1onwi"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as mplcolors\n","import sklearn.datasets as skdatasets\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"Mew5zEZjonwj"},"source":["# 2. Softmax regression (multi-class classification)"]},{"cell_type":"markdown","metadata":{"id":"RxurLELVonwj"},"source":["### 2.1. Load dataset"]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1664845206071,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"Ojkvexcronwk"},"outputs":[],"source":["def load_iris(normalize):\n","    # import data\n","    X, y = skdatasets.load_iris(return_X_y=True)\n","    # only use 1st & 3rd features\n","    X = X[:, [0, 2]]\n","    if normalize:\n","        X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n","    return train_test_split(X, y, test_size=0.2, random_state=444)\n","\n","def plot_iris(X, y, x1_mesh=None, x2_mesh=None, y_mesh=None):\n","    x1, x2 = X[:, 0], X[:, 1]\n","    plt.figure(figsize=(10, 6))\n","    plt.scatter(x1[y==0], x2[y==0], c='tab:blue', marker='o', label='Setosa')\n","    plt.scatter(x1[y==1], x2[y==1], c='tab:orange', marker='^', label='Versicolour')\n","    plt.scatter(x1[y==2], x2[y==2], c='tab:green', marker='x', label='Virginica')\n","    if x1_mesh is not None:\n","        cmap = mplcolors.ListedColormap(['tab:blue', 'tab:orange', 'tab:green'])\n","        plt.contourf(x1_mesh, x2_mesh, y_mesh, levels=3, cmap=cmap, alpha=0.3)\n","    plt.title('Softmax Regression (Iris dataset)', fontsize=18, pad=12)\n","    plt.xlabel('x1: Sepal length', fontsize=16, labelpad=12) # 1st feat\n","    plt.ylabel('x2: Petal length', fontsize=16, labelpad=12) # 3rd feat\n","    plt.legend(fontsize=14, loc='upper left')\n","    plt.show()"]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1664845206071,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"R_KxpAnnonwl"},"outputs":[],"source":["x_train, x_test, y_train, y_test = load_iris(normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"executionInfo":{"elapsed":387,"status":"ok","timestamp":1664845206454,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"nN7YneCUonwl","outputId":"4c81b218-72d4-4bd3-cc7e-35e84e74b012"},"outputs":[],"source":["plot_iris(x_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"d9AcVbRSonwm"},"source":["### 2.2. Affine, softmax, cross_entropy"]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1664845208230,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"g3RrtuaQonwm"},"outputs":[],"source":["def affine(x, w, b):\n","    out = None\n","    #########################################################################\n","    # TODO: Implement the affine transformation. Store the result in the    #\n","    # out variable.                                                         #\n","    #########################################################################\n","    # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n","    out = np.dot(x, w) + b\n","    # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n","    cache = (x, w, b)\n","    return out, cache\n","\n","def affine_backward(dout, cache):\n","    x, w, b = cache\n","    dx, dw, db = None, None, None\n","    #########################################################################\n","    # TODO: Implement the backward pass of the affine transformation.       #\n","    # Compute the gradient w.r.t inputs, weights, and biases. Store each of #\n","    # them in the dx, dw, db variable.                                      #\n","    #########################################################################\n","    # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n","    dw = np.dot(x.T, dout)\n","    db = np.sum(dout, axis=0)\n","    dx = np.dot(dout, w.T)\n","    # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n","    return dx, dw, db\n","\n","def softmax(x):\n","    yhat = None\n","    #########################################################################\n","    # TODO: Implement the softmax function without using explicit loops.    #\n","    # Store the softmax scores in the yhat variable.                        #\n","    #########################################################################\n","    # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n","    x = x - np.max(x, axis=1)[:, None]\n","    yhat = np.exp(x) / np.sum(np.exp(x), axis=1)[:, None]\n","    # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n","    return yhat\n","\n","def one_hot(x, n_class):\n","\treturn np.eye(n_class)[x]\n","\n","def cross_entropy_loss(yhat, y, n_class):\n","    # clip values to prevent divide by zero\n","    yhat = np.clip(yhat, a_min=1e-7, a_max=1-1e-7)\n","\n","    loss = None\n","    #########################################################################\n","    # TODO: Implement the cross-entropy function. Store the cross-entropy   #\n","    # loss in the loss variable. Note that the yhat is the output of the    #\n","    # softmax function implemented above.                                   #\n","    #########################################################################\n","    # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n","    loss = -np.mean(np.sum(np.log(yhat) * one_hot(y, n_class), axis=1))\n","    # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n","    \n","    return loss\n","\n","def cross_entropy_loss_and_grad(x, y, n_class):\n","    yhat = softmax(x)\n","    loss = cross_entropy_loss(yhat, y, n_class)\n","    grad = yhat - one_hot(y, n_class)\n","    return loss, grad"]},{"cell_type":"markdown","metadata":{"id":"D2__HODDonwo"},"source":["### 2.3. Train model"]},{"cell_type":"code","execution_count":61,"metadata":{"executionInfo":{"elapsed":329,"status":"ok","timestamp":1664845210235,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"eOt0Ev_Ponwo"},"outputs":[],"source":["def train_softmax_regression(x_train, x_test, y_train, y_test, lr, epochs, log_every):\n","    # set seed\n","    np.random.seed(444)\n","    # shape\n","    _, in_features = x_train.shape\n","    out_features = len(np.unique(y_train))\n","    # initialize weights\n","    w = np.random.rand(in_features, out_features)\n","    b = np.zeros(out_features)\n","    # begin training\n","    for e in range(1, epochs+1):\n","        # forward affine (out = wx + b)\n","        out, cache = affine(x_train, w, b)\n","        # compute cross-entropy loss and grad (dL/dout)\n","        loss, dout = cross_entropy_loss_and_grad(out, y_train, out_features)\n","        # backward affine\n","        _, dw, db = affine_backward(dout, cache)\n","        # gradient descent\n","        w -= lr * dw\n","        b -= lr * db\n","        # print log\n","        if e % log_every == 0:\n","            yhat = softmax(affine(x_test, w, b)[0])\n","            accr = np.mean(np.argmax(yhat, axis=1) == y_test)\n","            loss = cross_entropy_loss(yhat, y_test, out_features)\n","            print(f'Epochs: {e}/{epochs}, loss (test): {loss:.4f}, accuracy (test): {accr:.4f}')\n","    return w, b"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1664845210532,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"PkK9_4UOonwo","outputId":"f919687c-e04e-48e6-f19d-fe2947115443"},"outputs":[],"source":["w, b = train_softmax_regression(x_train, x_test, y_train, y_test, lr=1e-2, epochs=200, log_every=20)"]},{"cell_type":"code","execution_count":63,"metadata":{"executionInfo":{"elapsed":311,"status":"ok","timestamp":1664845212145,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"h1PbHWi6onwp"},"outputs":[],"source":["def compute_decision_boundary(x, w, b, resolution):\n","    x1, x2 = x[:, 0], x[:, 1]\n","    x1_mesh = np.linspace(np.min(x1) - 0.1, np.max(x1) + 0.1, resolution)\n","    x2_mesh = np.linspace(np.min(x2) - 0.1, np.max(x2) + 0.1, resolution)\n","    x1_mesh, x2_mesh = np.meshgrid(x1_mesh, x2_mesh, indexing='xy')\n","    x = np.c_[x1_mesh.ravel(), x2_mesh.ravel()]\n","    y_mesh = np.argmax(softmax(affine(x, w, b)[0]), axis=1)\n","    y_mesh = y_mesh.reshape(resolution, resolution)\n","    return x1_mesh, x2_mesh, y_mesh"]},{"cell_type":"code","execution_count":64,"metadata":{"executionInfo":{"elapsed":421,"status":"ok","timestamp":1664845214065,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"xuJrB8kQonwp"},"outputs":[],"source":["x1_mesh, x2_mesh, y_mesh = compute_decision_boundary(x_test, w, b, resolution=500)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"executionInfo":{"elapsed":823,"status":"ok","timestamp":1664845214885,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"BY8hTy7honwp","outputId":"e0147a27-ed2d-4b56-b4b8-2acc4544b89a"},"outputs":[],"source":["plot_iris(x_test, y_test, x1_mesh, x2_mesh, y_mesh)"]},{"cell_type":"markdown","metadata":{"id":"pptSzAJuonwp"},"source":["# 3. Multi-layer perceptron (MLP)"]},{"cell_type":"markdown","metadata":{"id":"GnlWcKvionwq"},"source":["### 3.1. Generate dataset (linearly non-separable)"]},{"cell_type":"code","execution_count":66,"metadata":{"executionInfo":{"elapsed":285,"status":"ok","timestamp":1664845219654,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"u1WLB1xDonwq"},"outputs":[],"source":["# from cs231n\n","# https://cs231n.github.io/neural-networks-case-study/\n","def generate_spiral_data(n_points, n_features, n_classes):\n","    np.random.seed(444)\n","    X = np.zeros((n_points*n_classes, n_features))\n","    y = np.zeros((n_points*n_classes,), dtype=np.uint8)\n","    for j in range(n_classes):\n","        i = range(n_points*j, n_points*(j+1))\n","        r = np.linspace(0., 1, n_points) # radius\n","        t = np.linspace(j*4, (j+1)*4, n_points) + np.random.randn(n_points) * 0.2 # theta\n","        X[i] = np.c_[r*np.sin(t), r*np.cos(t)]\n","        y[i] = j\n","    return train_test_split(X, y, test_size=0.2, random_state=444)\n","\n","def plot_spiral(X, y, x1_mesh=None, x2_mesh=None, x3_mesh=None):\n","    x1, x2 = X[:, 0], X[:, 1]\n","    plt.figure(figsize=(10, 6))\n","    plt.scatter(x1[y==0], x2[y==0], c='tab:blue', marker='o', label='class:0')\n","    plt.scatter(x1[y==1], x2[y==1], c='tab:orange', marker='^', label='class:1')\n","    plt.scatter(x1[y==2], x2[y==2], c='tab:green', marker='x', label='class:2')\n","    if x1_mesh is not None:\n","        cmap = mplcolors.ListedColormap(['tab:blue', 'tab:orange', 'tab:green'])\n","        plt.contourf(x1_mesh, x2_mesh, y_mesh, levels=3, cmap=cmap, alpha=0.3)\n","    plt.title('MLP (Spiral dataset)', fontsize=18, pad=12)\n","    plt.xlabel('x1', fontsize=16, labelpad=12)\n","    plt.ylabel('x2', fontsize=16, labelpad=12)\n","    plt.legend(fontsize=14, loc='upper left')\n","    plt.show()"]},{"cell_type":"code","execution_count":67,"metadata":{"executionInfo":{"elapsed":402,"status":"ok","timestamp":1664845220490,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"2GwP2gvFonwq"},"outputs":[],"source":["x_train, x_test, y_train, y_test = generate_spiral_data(n_points=100, n_features=2, n_classes=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"executionInfo":{"elapsed":414,"status":"ok","timestamp":1664845220899,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"bsH1kRgwonwq","outputId":"e892bea0-d239-4cd7-b47e-eedd385f3364"},"outputs":[],"source":["plot_spiral(x_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"GiLoWfFionwr"},"source":["### 3.2. MLP implementation"]},{"cell_type":"markdown","metadata":{"id":"If9lSZ1gonwr"},"source":["* **Q1** (Linear layer): 4 points; 1 point each"]},{"cell_type":"code","execution_count":69,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1664845223292,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"sQb6hLUXonwr"},"outputs":[],"source":["class Linear:\n","    def __init__(self, in_features, out_features, is_first=False):\n","        #########################################################################\n","        # TODO: Initialize the weights and biases of the linear layer. Store    #\n","        # the weights in the self.w variable and biases in the self.b variable. #\n","        # Weights should be a 2d np.ndarray of shape (in_features, out_features)#\n","        # and initialized from a Uniform ranged [0, 1]. Biases should be a 1d   #\n","        # np.ndarray of shape (out_features,) and initialized to zero.          #\n","        #########################################################################\n","        # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n","        \n","        pass\n","\n","        # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n","        self.dw = None\n","        self.db = None\n","        self.is_first = is_first\n","    \n","    def __call__(self, x):\n","        out = None\n","        self.input = x # cache for backward\n","        #########################################################################\n","        # TODO: Implement the forward pass for the linear layer. Compute the    #\n","        # affine transform of X and store the result in the out variable.       #\n","        #########################################################################\n","        # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n","        \n","        pass\n","\n","        # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n","        return out\n","    \n","    def backward(self, dout):\n","        #########################################################################\n","        # TODO: Implement the backward pass for the linear layer. Store the     #\n","        # gradient of loss w.r.t weights (dL/dw) in the self.dw variable, and   #\n","        # the gradient of loss w.r.t biases (dL/db) in the self.db variable.    #\n","        #########################################################################\n","        # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n","        \n","        pass\n","\n","        # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n","\n","        dx = None\n","        #########################################################################\n","        # TODO: Implement the backward pass for the linear layer. Store the     #\n","        # gradient of loss w.r.t inputs (dL/dx) in the dx variable. Note that   #\n","        # we compute dx, if this is not the first layer (i.e., if self.is_first #\n","        # is False)                                                             #\n","        #########################################################################\n","        # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n","\n","        if not self.is_first:\n","            pass\n","        \n","        # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n","\n","        return dx"]},{"cell_type":"markdown","metadata":{"id":"uTaVfbDConwr"},"source":["* **Q2** (ReLU layer): 2 points; 1 point each"]},{"cell_type":"code","execution_count":70,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1664845226720,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"_NsGXVDXonws"},"outputs":[],"source":["class ReLU:\n","    def __call__(self, x):\n","        self.input = x # cache for backward\n","\n","        out = None\n","        #########################################################################\n","        # TODO: Implement the forward pass of ReLU layer. Store the computed    #\n","        # values in the out variable                                            #\n","        #########################################################################\n","        # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n","\n","        pass\n","        \n","        # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n","        return out\n","    \n","    def backward(self, dout):\n","        dx = None\n","        #########################################################################\n","        # TODO: Implement the backward pass of ReLU layer. Store the computed   #\n","        # values in the dx variable. Note that you can use self.input to        #\n","        # compute the gradients.                                                #\n","        #########################################################################\n","        # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n","\n","        pass\n","\n","        # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n","        return dx"]},{"cell_type":"markdown","metadata":{"id":"HIQYK1uqonws"},"source":["* MLP (no need to implement)"]},{"cell_type":"code","execution_count":71,"metadata":{"executionInfo":{"elapsed":448,"status":"ok","timestamp":1664845230975,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"PUm4dHtDonws"},"outputs":[],"source":["class MLP:\n","    def __init__(self, features):\n","        def setup_layers(features):\n","            layers = [] # n: in_features, m: out_features\n","            for i, (n, m) in enumerate(zip(features[:-1], features[1:])):\n","                layers += [Linear(n, m, i==0), ReLU()]\n","            del layers[-1]\n","            return layers\n","        \n","        self.layers = setup_layers(features)\n","    \n","    def __call__(self, X):\n","        for layer in self.layers:\n","            X = layer(X)\n","        return X\n","    \n","    def backward(self, dout):\n","        for layer in reversed(self.layers):\n","            dout = layer.backward(dout)"]},{"cell_type":"markdown","metadata":{"id":"OLZrnXBOonws"},"source":["* **Q3** (Cross-entropy layer): 2 points"]},{"cell_type":"code","execution_count":72,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1664845231257,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"wQBQG9Wyonws"},"outputs":[],"source":["class CrossEntropyLoss:\n","    def __init__(self, n_class):\n","        self.n_class = n_class\n","    \n","    def __call__(self, x, y):\n","        loss, grad = None, None\n","        #########################################################################\n","        # TODO: Compute the cross-entropy loss and its gradient without using   #\n","        # explicit loops. Store the loss in loss variable and the gradient in   #\n","        # grad variable. Please use the softmax and cross_entropy_loss function #\n","        # implemented above. Hint: this function is equivalent to the           #\n","        # cross_entropy_loss_and_grad() implemented above.                      #\n","        #########################################################################\n","        # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n","        \n","        pass\n","\n","        # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #\n","        return loss, grad"]},{"cell_type":"markdown","metadata":{"id":"7iieEJaKonws"},"source":["* **Q4** (Gradient descent): 2 points"]},{"cell_type":"code","execution_count":73,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1664845232336,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"6b-DI4gjonws"},"outputs":[],"source":["class GradientDescent:\n","    def __init__(self, layers, lr):\n","        self.layers = layers\n","        self.lr = lr\n","\n","    def step(self):\n","        #########################################################################\n","        # TODO: Implement the gradient descent algorithm. Note that self.layers #\n","        # is a list of layers, where each of them could be either Linear or     # \n","        # ReLU. Since a ReLU layer doesn't have learnable parameters, we only   #\n","        # update the parameters of the Linear layers. Loop through self.layers  #\n","        # and update the parameters if the layer is instance of Linear class.   #\n","        #########################################################################\n","        # ******** START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ******** #\n","        \n","        pass\n","\n","        # ********* END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE) ********* #"]},{"cell_type":"markdown","metadata":{"id":"QkJaJddzonwt"},"source":["* main function (no need to implement)"]},{"cell_type":"code","execution_count":74,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1664845233631,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"AX0kUiklonwt"},"outputs":[],"source":["def train_mlp(x_train, x_test, y_train, y_test, features, lr, epochs, log_every):\n","    # set seed\n","    np.random.seed(444)\n","\n","    # make a single linear layer\n","    model = MLP(features)\n","\n","    # make optimizer\n","    optim = GradientDescent(model.layers, lr)\n","\n","    # loss function\n","    n_class = len(np.unique(y_train))\n","    loss_fn = CrossEntropyLoss(n_class)\n","\n","    # evaluation function\n","    eval_fn = lambda yhat, y: np.mean(np.argmax(yhat, axis=1) == y)\n","\n","    # begin training\n","    for e in range(1, epochs+1):\n","        # forward (linear layer)\n","        out = model(x_train)\n","        # compute cross-entropy loss and gradient w.r.t out\n","        loss, dout = loss_fn(out, y_train)\n","        # backward (linear layer)\n","        model.backward(dout)\n","        # gradient descent\n","        optim.step()\n","        # print log\n","        if e % log_every == 0:\n","            yhat = softmax(model(x_test))\n","            loss = cross_entropy_loss(yhat, y_test, n_class)\n","            accr = eval_fn(yhat, y_test)\n","            print(f'Epochs: {e}/{epochs}, loss (test): {loss:.4f}, accuracy (test): {accr:.4f}')\n","    \n","    return model\n","\n","def compute_decision_boundary_mlp(x, model, resolution):\n","    x1, x2 = x[:, 0], x[:, 1]\n","    x1_mesh = np.linspace(np.min(x1) - 0.1, np.max(x1) + 0.1, resolution)\n","    x2_mesh = np.linspace(np.min(x2) - 0.1, np.max(x2) + 0.1, resolution)\n","    x1_mesh, x2_mesh = np.meshgrid(x1_mesh, x2_mesh, indexing='xy')\n","    x = np.c_[x1_mesh.ravel(), x2_mesh.ravel()]\n","    y_mesh = np.argmax(softmax(model(x)), axis=1).reshape(resolution, resolution)\n","    return x1_mesh, x2_mesh, y_mesh"]},{"cell_type":"markdown","metadata":{"id":"wtdoUEVQonwt"},"source":["### 3.3. Train single layer perceptron"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1664845234853,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"W9wksyOtonwt","outputId":"30dd3817-f786-454a-dc0c-0f564fe87d86"},"outputs":[],"source":["slp = train_mlp(x_train, x_test, y_train, y_test, features=[2, 3], lr=1e-2, epochs=200, log_every=20)"]},{"cell_type":"code","execution_count":76,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1664845235636,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"zr6-tqVdonwt"},"outputs":[],"source":["x1_mesh, x2_mesh, y_mesh = compute_decision_boundary_mlp(x_test, slp, resolution=500)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"executionInfo":{"elapsed":313,"status":"ok","timestamp":1664845237097,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"tyw5uxQgonwt","outputId":"dabb2eb6-cb3f-44a5-ceb4-de7c7997f6ae"},"outputs":[],"source":["plot_spiral(x_test, y_test, x1_mesh, x2_mesh, y_mesh)"]},{"cell_type":"markdown","metadata":{"id":"htmb4CNQonwu"},"source":["### 3.4. Train multi layer perceptron"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1664845238750,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"0JePAmajonwu","outputId":"8e5bbdec-780e-4f17-c9c4-f30c3a37dead"},"outputs":[],"source":["mlp = train_mlp(x_train, x_test, y_train, y_test, features=[2, 10, 3], lr=1e-2, epochs=500, log_every=50)"]},{"cell_type":"code","execution_count":79,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1664845240726,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"N3BISz7ronwu"},"outputs":[],"source":["x1_mesh, x2_mesh, y_mesh = compute_decision_boundary_mlp(x_test, mlp, resolution=500)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":427},"executionInfo":{"elapsed":356,"status":"ok","timestamp":1664845241380,"user":{"displayName":"인공지능학과/남승태","userId":"04895831295337085254"},"user_tz":-540},"id":"COPp19c8onwu","outputId":"ae9e4995-a5be-4698-f9c2-03a13867f527"},"outputs":[],"source":["plot_spiral(x_test, y_test, x1_mesh, x2_mesh, y_mesh)"]},{"cell_type":"markdown","metadata":{"id":"tg4bpxqAonwu"},"source":["# 4. Optional readings\n","\n","*   [Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.7.11 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.11"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"}}},"nbformat":4,"nbformat_minor":0}
